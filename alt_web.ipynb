{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I was able to take the CURL commands on the smat API website and rewrite them into python. Here is what that looks like. Here, I can call in data from compatible SMAT websites into python and analyze them in different ways. For exmample, the JSON output below includes all counts of polls on .win websites for the past month (shown in the doc_count). This is timeseries data, as denoted by the url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_key': 'timestamp', 'took': 770, 'timed_out': False, '_shards': {'total': 16, 'successful': 16, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1013, 'relation': 'eq'}, 'max_score': None, 'hits': []}, 'aggregations': {'timestamp': {'buckets': [{'key_as_string': '11/12/22(Sat)00:00:00', 'key': 1668211200000, 'doc_count': 74}, {'key_as_string': '11/13/22(Sun)00:00:00', 'key': 1668297600000, 'doc_count': 78}, {'key_as_string': '11/14/22(Mon)00:00:00', 'key': 1668384000000, 'doc_count': 92}, {'key_as_string': '11/15/22(Tue)00:00:00', 'key': 1668470400000, 'doc_count': 89}, {'key_as_string': '11/16/22(Wed)00:00:00', 'key': 1668556800000, 'doc_count': 66}, {'key_as_string': '11/17/22(Thu)00:00:00', 'key': 1668643200000, 'doc_count': 29}, {'key_as_string': '11/18/22(Fri)00:00:00', 'key': 1668729600000, 'doc_count': 26}, {'key_as_string': '11/19/22(Sat)00:00:00', 'key': 1668816000000, 'doc_count': 36}, {'key_as_string': '11/20/22(Sun)00:00:00', 'key': 1668902400000, 'doc_count': 29}, {'key_as_string': '11/21/22(Mon)00:00:00', 'key': 1668988800000, 'doc_count': 14}, {'key_as_string': '11/22/22(Tue)00:00:00', 'key': 1669075200000, 'doc_count': 31}, {'key_as_string': '11/23/22(Wed)00:00:00', 'key': 1669161600000, 'doc_count': 26}, {'key_as_string': '11/24/22(Thu)00:00:00', 'key': 1669248000000, 'doc_count': 10}, {'key_as_string': '11/25/22(Fri)00:00:00', 'key': 1669334400000, 'doc_count': 32}, {'key_as_string': '11/26/22(Sat)00:00:00', 'key': 1669420800000, 'doc_count': 15}, {'key_as_string': '11/27/22(Sun)00:00:00', 'key': 1669507200000, 'doc_count': 13}, {'key_as_string': '11/28/22(Mon)00:00:00', 'key': 1669593600000, 'doc_count': 16}, {'key_as_string': '11/29/22(Tue)00:00:00', 'key': 1669680000000, 'doc_count': 17}, {'key_as_string': '11/30/22(Wed)00:00:00', 'key': 1669766400000, 'doc_count': 13}, {'key_as_string': '12/1/22(Thu)00:00:00', 'key': 1669852800000, 'doc_count': 28}, {'key_as_string': '12/2/22(Fri)00:00:00', 'key': 1669939200000, 'doc_count': 14}, {'key_as_string': '12/3/22(Sat)00:00:00', 'key': 1670025600000, 'doc_count': 25}, {'key_as_string': '12/4/22(Sun)00:00:00', 'key': 1670112000000, 'doc_count': 42}, {'key_as_string': '12/5/22(Mon)00:00:00', 'key': 1670198400000, 'doc_count': 36}, {'key_as_string': '12/6/22(Tue)00:00:00', 'key': 1670284800000, 'doc_count': 70}, {'key_as_string': '12/7/22(Wed)00:00:00', 'key': 1670371200000, 'doc_count': 57}, {'key_as_string': '12/8/22(Thu)00:00:00', 'key': 1670457600000, 'doc_count': 14}, {'key_as_string': '12/9/22(Fri)00:00:00', 'key': 1670544000000, 'doc_count': 10}, {'key_as_string': '12/10/22(Sat)00:00:00', 'key': 1670630400000, 'doc_count': 9}, {'key_as_string': '12/11/22(Sun)00:00:00', 'key': 1670716800000, 'doc_count': 2}]}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://api.smat-app.com/timeseries\"\n",
    "\n",
    "params = {\n",
    "    \"term\": \"polls\",\n",
    "    \"interval\": \"day\",\n",
    "    \"site\": \"win\",\n",
    "    \"since\": \"2022-11-12\",\n",
    "    \"until\": \"2022-12-12\",\n",
    "    \"changepoint\": \"false\",\n",
    "    \"esquery\": \"false\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "df = response.json()\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we have another use case of the smat API: text. This will probably the most useful for my analysis. Here, I actually figured out how to transform this into a dataframe. From what I can tell, it doesn't seem to have a limit. I have tested up to 100,000 rows. ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community</th>\n",
       "      <th>content</th>\n",
       "      <th>author</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>It's the company not Trump. Trump is not being...</td>\n",
       "      <td>Agent_86</td>\n",
       "      <td>2022-12-07 00:46:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Trump\\n</td>\n",
       "      <td>BasedBabe1776</td>\n",
       "      <td>2022-12-07 02:33:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>trump\\n</td>\n",
       "      <td>gabman</td>\n",
       "      <td>2022-12-07 11:24:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Walker was endorsed because of his history wit...</td>\n",
       "      <td>GhostOfJebsCampaign</td>\n",
       "      <td>2022-12-07 04:04:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>thanks Trump\\n</td>\n",
       "      <td>cyberwar</td>\n",
       "      <td>2022-12-07 10:45:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>Because this isn't the first time these exact ...</td>\n",
       "      <td>cathole953</td>\n",
       "      <td>2022-12-07 15:10:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>If you see an article/video here on Unleashed ...</td>\n",
       "      <td>ashlanddog</td>\n",
       "      <td>2022-12-07 05:01:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>No End to the Corruption\\nJeffery Tucker\\nJEFF...</td>\n",
       "      <td>BKav</td>\n",
       "      <td>2022-12-07 03:06:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>How Long Will Elon Survive?\\nJeffery Tucker\\nJ...</td>\n",
       "      <td>BKav</td>\n",
       "      <td>2022-12-07 23:04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>No End to the Corruption\\nJeffery Tucker\\nJEFF...</td>\n",
       "      <td>BKav</td>\n",
       "      <td>2022-12-07 03:06:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1230 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           community                                            content  \\\n",
       "0          thedonald  It's the company not Trump. Trump is not being...   \n",
       "1          thedonald                                            Trump\\n   \n",
       "2          thedonald                                            trump\\n   \n",
       "3          thedonald  Walker was endorsed because of his history wit...   \n",
       "4          thedonald                                     thanks Trump\\n   \n",
       "...              ...                                                ...   \n",
       "1225  greatawakening  Because this isn't the first time these exact ...   \n",
       "1226  greatawakening  If you see an article/video here on Unleashed ...   \n",
       "1227       thedonald  No End to the Corruption\\nJeffery Tucker\\nJEFF...   \n",
       "1228       thedonald  How Long Will Elon Survive?\\nJeffery Tucker\\nJ...   \n",
       "1229       thedonald  No End to the Corruption\\nJeffery Tucker\\nJEFF...   \n",
       "\n",
       "                   author           timestamp  \n",
       "0                Agent_86 2022-12-07 00:46:39  \n",
       "1           BasedBabe1776 2022-12-07 02:33:53  \n",
       "2                  gabman 2022-12-07 11:24:48  \n",
       "3     GhostOfJebsCampaign 2022-12-07 04:04:49  \n",
       "4                cyberwar 2022-12-07 10:45:58  \n",
       "...                   ...                 ...  \n",
       "1225           cathole953 2022-12-07 15:10:42  \n",
       "1226           ashlanddog 2022-12-07 05:01:42  \n",
       "1227                 BKav 2022-12-07 03:06:05  \n",
       "1228                 BKav 2022-12-07 23:04:04  \n",
       "1229                 BKav 2022-12-07 03:06:05  \n",
       "\n",
       "[1230 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://api.smat-app.com/content\"\n",
    "\n",
    "params = {\n",
    "    \"term\": \"trump\",\n",
    "    \"limit\": 5000,\n",
    "    \"site\": \"win\",\n",
    "    \"since\": \"2022-12-07\",\n",
    "    \"until\": \"2022-12-08\",\n",
    "    \"esquery\": \"false\",\n",
    "    \"sortdesc\": \"false\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "responses = requests.get(url, params=params, headers=headers)\n",
    "json_output = responses.json()\n",
    "\n",
    "#parse through JSON files and make 'content', 'author, and 'timestamp' a column\n",
    "df2 = pd.DataFrame(columns=['community','content','author','timestamp'])\n",
    "for hit in json_output['hits']['hits']:\n",
    "    community = hit['_source']['community']\n",
    "    content = hit['_source']['content']\n",
    "    author = hit['_source']['author']\n",
    "    timestamp = hit['_source']['timestamp']\n",
    "    df2.loc[len(df2)] = [community, content, author,timestamp]\n",
    "\n",
    "#convert timestamp column to readable time format\n",
    "df2['timestamp'] = pd.to_datetime(df2['timestamp'], unit='s')\n",
    "\n",
    "#remove <p>'s from text\n",
    "df2['content'] = df2['content'].str.replace('<p>', '')\n",
    "df2['content'] = df2['content'].str.replace('</p>', '')\n",
    "display(df2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, I use the timeseries data and transform it into a dataframe. This parsing was a bit easier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/12/22(Wed)</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/13/22(Thu)</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/14/22(Fri)</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/15/22(Sat)</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/16/22(Sun)</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>12/7/22(Wed)</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>12/8/22(Thu)</td>\n",
       "      <td>623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>12/9/22(Fri)</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>12/10/22(Sat)</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>12/11/22(Sun)</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  mentions\n",
       "0   10/12/22(Wed)       715\n",
       "1   10/13/22(Thu)       779\n",
       "2   10/14/22(Fri)       714\n",
       "3   10/15/22(Sat)       601\n",
       "4   10/16/22(Sun)       606\n",
       "..            ...       ...\n",
       "56   12/7/22(Wed)      1230\n",
       "57   12/8/22(Thu)       623\n",
       "58   12/9/22(Fri)       479\n",
       "59  12/10/22(Sat)       622\n",
       "60  12/11/22(Sun)       562\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://api.smat-app.com/timeseries\"\n",
    "\n",
    "params = {\n",
    "    \"term\": \"trump\",\n",
    "    \"interval\": \"day\",\n",
    "    \"site\": \"win\",\n",
    "    \"since\": \"2022-10-12\",\n",
    "    \"until\": \"2022-12-12\",\n",
    "    \"changepoint\": \"false\",\n",
    "    \"esquery\": \"false\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "json_data = response.json()\n",
    "\n",
    "\n",
    "buckets = json_data['aggregations']['timestamp']['buckets']\n",
    "\n",
    "#dataframe\n",
    "df3 = pd.DataFrame(buckets)\n",
    "\n",
    "#make the df more legible\n",
    "df3 = df3.drop('key', axis=1)\n",
    "df3['key_as_string'] = df3['key_as_string'].str.replace('00:00:00', '')\n",
    "df3 = df3.rename(columns={'key_as_string': 'date', 'doc_count': 'mentions'})\n",
    "\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately, I found out that 'site' actually has many different params (rumble_video, rumble_comment, bitchute_video, bitchute_comment, rutube_video, rutube_comment, tiktok_video, tiktok_comment, lbry_video, lbry_comment, 8kun, 4chan, gab, parler, win, poal, telegram, kiwifarms, gettr, wimkin, mewe, minds, vk, truth_social) and that their JSON files need to be parsed differently. I will eventually go through all of this but, as you can see, the buckets variable is slightly different, using 'now' in the dict rather than 'timestamp' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             date  mentions\n",
      "0   10/10/22(Mon)       927\n",
      "1   10/11/22(Tue)      1079\n",
      "2   10/12/22(Wed)      1009\n",
      "3   10/13/22(Thu)      1307\n",
      "4   10/14/22(Fri)       239\n",
      "..            ...       ...\n",
      "58   12/7/22(Wed)       616\n",
      "59   12/8/22(Thu)       553\n",
      "60   12/9/22(Fri)       506\n",
      "61  12/10/22(Sat)       509\n",
      "62  12/11/22(Sun)       521\n",
      "\n",
      "[63 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.smat-app.com/timeseries\"\n",
    "\n",
    "params = {\n",
    "    \"term\": \"trump\",\n",
    "    \"interval\": \"day\",\n",
    "    \"site\": \"4chan\",\n",
    "    \"since\": \"2022-10-10\",\n",
    "    \"until\": \"2022-12-12\",\n",
    "    \"changepoint\": \"false\",\n",
    "    \"esquery\": \"false\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "json_datas = response.json()\n",
    "\n",
    "buckets = json_datas['aggregations']['now']['buckets']\n",
    "df4 = pd.DataFrame(buckets)\n",
    "\n",
    "#drop columns and make date more readable\n",
    "df4 = df4.drop('key', axis=1)\n",
    "df4['key_as_string'] = df4['key_as_string'].str.replace('00:00:00', '')\n",
    "df4 = df4.rename(columns={'key_as_string': 'date', 'doc_count': 'mentions'})\n",
    "display(df4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP for truth_social parsing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_key': 'created_at', 'content_key': 'content_cleaned', 'took': 1666, 'timed_out': False, '_shards': {'total': 7, 'successful': 7, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 96, 'relation': 'eq'}, 'max_score': 9.862146, 'hits': [{'_index': 'smat-truthsocial-data-000007', '_id': '109480411827955943', '_score': 9.862146, '_source': {'account': {'acct': 'AZChris11', 'display_name': 'AZChris', 'id': '107838368043245844', 'username': 'AZChris11'}, 'bookmarked': False, 'card': None, 'collected_by': 'smat-scrapy-crawlers', 'content': '<p>Nice!!!</p>', 'content_cleaned': 'Nice!!!', 'created_at': '2022-12-08T22:28:30.554+00:00', 'datatype': 'comment', 'emojis': [], 'favourited': False, 'favourites_count': 0, 'id': '109480411827955943', 'in_reply_to_account_id': '107804517036100106', 'in_reply_to_id': '109480126795920446', 'language': 'en', 'last_seen_ts': '2022-12-11T07:19:34.698368+00:00', 'media_attachments': [], 'mentions': [{'acct': 'dbongino', 'id': '107804517036100106', 'url': 'https://truthsocial.com/@dbongino', 'username': 'dbongino'}], 'muted': False, 'poll': None, 'quote': None, 'reblogged': False, 'reblogs_count': 0, 'replies_count': 0, 'sensitive': False, 'spoiler_text': '', 'tags': [], 'uri': 'https://truthsocial.com/users/AZChris11/statuses/109480411827955943', 'url': 'https://truthsocial.com/@AZChris11/109480411827955943', 'visibility': 'public'}}]}}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.smat-app.com/content\"\n",
    "\n",
    "params = {\n",
    "    \"term\": \"nice\",\n",
    "    \"limit\": 1,\n",
    "    \"site\": \"truth_social\",\n",
    "    \"since\": \"2022-12-08\",\n",
    "    \"until\": \"2022-12-09\",\n",
    "    \"esquery\": \"false\",\n",
    "    \"sortdesc\": \"false\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "responses = requests.get(url, params=params, headers=headers)\n",
    "firey = responses.json()\n",
    "print(firey)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End goal is to combine ALL websites (or select a handful of websites) into a single dataframe. Doable, just a little more work needed from my end. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
